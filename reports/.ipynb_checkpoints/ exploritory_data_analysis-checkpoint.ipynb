{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'automatic_medical_coding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1cf00594f875>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrich\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mautomatic_medical_coding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mautomatic_medical_coding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTARGET_COLUMN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mID_COLUMN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_analysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwandb_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_runs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'automatic_medical_coding'"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from rich.progress import track\n",
    "\n",
    "import automatic_medical_coding.metrics as metrics\n",
    "from automatic_medical_coding.settings import TARGET_COLUMN, ID_COLUMN\n",
    "from data_analysis.wandb_utils import get_runs, get_run\n",
    "\n",
    "\n",
    "mimic_clean = pd.read_feather(\"/data/je/mimiciii/pre-processed/clean/mimiciii_clean.feather\")\n",
    "mimic_clean_splits = pd.read_feather(\"/data/je/mimiciii/pre-processed/clean/mimiciii_clean.feather\")\n",
    "code_columns = [\"icd9_diag\" ,\"icd9_proc\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "/home/je/.local/lib/python3.8/site-packages/cryptography/hazmat/backends/openssl/x509.py:14: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  warnings.warn(\n",
      "Collecting rich\n",
      "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
      "\u001b[K     |████████████████████████████████| 237 kB 5.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /opt/anaconda3/lib/python3.8/site-packages (from rich) (2.6.1)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0; python_version < \"3.9\" in /home/je/.local/lib/python3.8/site-packages (from rich) (4.3.0)\n",
      "Collecting commonmark<0.10.0,>=0.9.0\n",
      "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
      "\u001b[K     |████████████████████████████████| 51 kB 15.2 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: commonmark, rich\n",
      "\u001b[33m  WARNING: The script cmark is installed in '/home/je/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed commonmark-0.9.1 rich-12.6.0\n",
      "/home/je/.local/lib/python3.8/site-packages/cryptography/hazmat/backends/openssl/x509.py:14: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"joakim_edin/automatic-medical-coding\"\n",
    "SWEEP_ID = \"rzzkucfv\"\n",
    "DATASET = \"mimiciii_clean\"\n",
    "\n",
    "EXPERIMENT_DIR = Path(\"/data/je/experiments/\")\n",
    "MODEL_NAMES = {\"PLMICD\": \"PLM-ICD\", \"VanillaCNN\": \"CNN\", \"VanillaRNN\": \"Bi-GRU\"}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def one_hot(\n",
    "    targets: list[list[str]], number_of_classes: int, target2index: dict[str, int]\n",
    ") -> torch.Tensor:\n",
    "    output_tensor = torch.zeros((len(targets), number_of_classes))\n",
    "    for idx, case in enumerate(targets):\n",
    "        for target in case:\n",
    "            if target in target2index:\n",
    "                output_tensor[idx, target2index[target]] = 1\n",
    "    return output_tensor.long()\n",
    "\n",
    "\n",
    "def load_predictions(run_id: str) -> tuple[dict[str, torch.Tensor], str, int]:\n",
    "    predictions = pd.read_feather(EXPERIMENT_DIR / run_id / \"predictions_val.feather\")\n",
    "\n",
    "    predictions[TARGET_COLUMN] = predictions[TARGET_COLUMN].apply(\n",
    "        lambda x: x.tolist()\n",
    "    )  # convert from numpy array to list\n",
    "    targets = predictions[[TARGET_COLUMN]]\n",
    "    unique_targets = list(set.union(*targets[TARGET_COLUMN].apply(set)))\n",
    "    logits = predictions[unique_targets]\n",
    "    target2index = {target: idx for idx, target in enumerate(unique_targets)}\n",
    "    number_of_classes = len(target2index)\n",
    "\n",
    "    # Mapping from target to index and vice versa\n",
    "    targets_torch = one_hot(\n",
    "        targets[TARGET_COLUMN].to_list(), number_of_classes, target2index\n",
    "    )  # one_hot encoding of targets\n",
    "    logits_torch = torch.tensor(logits.values)\n",
    "    cases = {\"logits\": logits_torch, \"targets\": targets_torch}\n",
    "    return cases, number_of_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coding",
   "language": "python",
   "name": "coding"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
